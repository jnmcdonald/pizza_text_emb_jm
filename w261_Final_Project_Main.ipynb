{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w261 Final Project Main",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "H2rf7b8cOFpI",
        "xmFB3kKYgCVa",
        "NLLOLxJ6i8x-"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jnmcdonald/pizza_text_emb_jm/blob/master/w261_Final_Project_Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "kNr86JNTA-Py",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# I. Question Formulation\n",
        "\n",
        "In this final project, we aim to predict whether an individual web surfer will click on a given display ad."
      ]
    },
    {
      "metadata": {
        "id": "WZDji-gaB7t5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is important for maximizing the bottom line of an advertising network (Criteo is the ad network that provided our final project dataset, originally released through a Kaggle competition). The company earns revenue when they serve the “right” ads to the “right” people, who sometimes click a compelling offer."
      ]
    },
    {
      "metadata": {
        "id": "A3-CdVLWB_r3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Displaying an inappropriate ad (a coupon for infant diapers to someone not a parent, for instance) is a lost opportunity to show that person a more relevant ad...and thus a missed opportunity to earn revenue."
      ]
    },
    {
      "metadata": {
        "id": "o_Qi_RyIDmMb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Project Justification"
      ]
    },
    {
      "metadata": {
        "id": "WRlwxqNfQOHn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What is the point of trying to quickly predict CTR for many ad-surfer combinations?\n",
        "\n",
        "Criteo states that they must rank eligible ads in real time, once a surfer’s device attempts to load a webpage. If their network can pick and display only the ad (or ads) with the highest likelihood of being clicked by the surfer, then they stand to maximize their advertising earnings. They also take into consideration the price an advertiser is willing/able to pay. So what ultimately determines the ad(s) shown will be a formula represented here:"
      ]
    },
    {
      "metadata": {
        "id": "UoReXhLviA_v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$$eCPM = (predicted\\ probability\\ of\\ a\\ click\\ by\\ this\\ surfer\\ on\\ this\\ ad) \\times (advertiser's\\ bid\\ in\\ dollars)$$\n"
      ]
    },
    {
      "metadata": {
        "id": "E7M5l4gzQRqQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our task in this project is to ensure that the first term - the predicted probability of a click occurring - is right as often as possible, so that better-fit ads don’t get passed over (left on the virtual cutting-room floor) while so-so or poor-fit ads get displayed to the user...who finds them irrelevant and ignores them. Because the outcome of the surfer’s interaction with the display ad is either a click or not a click, we are predicting a binary variable, which is something to which Logistic Regression is particularly well-suited."
      ]
    },
    {
      "metadata": {
        "id": "LnoJ8viZQT-0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How will we know if our model is useful?\n",
        "\n",
        "Well, one can be *accurate* 74% of the time with this dataset by simply guessing that the user will not click on any add at any time. That’s because the CTR overall is just about 26%. This is not a useful observation, because an algorithm that predicts users never click will serve no display ads, and earn no revenue.  \n",
        "\n",
        "We found that the Area Under the Receiver Operating Characteristic curve (AUC) performance measure was commonly used in the industry. Because the ROC curve plots basically the true positive rate versus the false positive rate, it focuses evaluation on keeping false positives to a minimum, while maximizing true positive identification. Etsy found that evaluating their algorithms with regard to AUC gave them “a reliable indicator of predictive power...[where] improvements in AUC (> 1%) have consistently resulted in significant CTR wins in multiple rounds of online A/B tests.” We focus therefore on maximizing our AUC, where perfect would be 1.0 or 100%. The best a random choice can do is 0.5 AUC, which means that when you say something will get a click, you’re right half the time and the other half it’s a false positive. So if we have a measure of 0.69, that means that when we say an ad will be clicked, we’re right almost 7 times out of 10…at least on the dev set. And we'll keep trying to beat this."
      ]
    },
    {
      "metadata": {
        "id": "Q73eEv1RCUeu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### About the Dataset"
      ]
    },
    {
      "metadata": {
        "id": "cU2WQ0CwQIrZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The provided dataset represents a sample of 1 week’s display ad records. The training dataset comes from the first 6 days of that week, and the test set comes from the 7th day’s ads. 13 numerical and 26 categorical fields are provided to “describe” each advertisement that was displayed to a surfer, and a label indicates whether the unknown surfer did in fact click (or not).\n",
        "\n",
        "Because click-through rates (CTR) are typically quite low (2-3% is actually a very high CTR), we suspect the data provided includes a higher ratio of positive examples than a true random sample, as the average CTR is just over 26% in the provided data. This is likely done to provide a higher ratio of positive examples for model training. It’s similar to how airport Transportation Security Administration (TSA) agents very rarely see a real threat, but most of their training materials (and time investment) focus on how to spot those rare events."
      ]
    },
    {
      "metadata": {
        "id": "aE-wyqU3QMBJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Because the numerical and categorical columns are not decipherable (Criteo did this on purpose to protect the anonymity of advertisers and surfers), we make only general assumptions about the kinds of information these represent. Count data is likely related to how many times an ad has been served and/or clicked-on, perhaps how many times the surfer had visited the site through which the ad was served, etc. Categorical data might include the URL where the ad was displayed, the advertiser’s brand, their industry (sports or music or retail, etc.), as well as tags related to the surfer and the particular ad’s tracking campaign, etc."
      ]
    },
    {
      "metadata": {
        "id": "vQQHqyMRvvVc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Section citations:\n",
        "* Criteo: https://labs.criteo.com/\n",
        "* Original Kaggle competition project page: https://www.kaggle.com/c/criteo-display-ad-challenge\n",
        "* The dataset - fully anonymized - was made available for academic usage after the competition concluded: http://labs.criteo.com/2014/09/kaggle-contest-dataset-now-available-academic-use/\n",
        "* Etsy paper by Aryafar et al, 2017; An Ensemble-Based Approach to Click-Through Rate Prediction for Promoted Listings at Etsy, https://arxiv.org/pdf/1711.01377.pdf\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "GYzTWDQlHY2r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# II. Algorithm Explanation"
      ]
    },
    {
      "metadata": {
        "id": "KAfxv7RPqaQD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ]
    },
    {
      "metadata": {
        "id": "RA7VJG7eHey3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are a number of algorithms that could be used to answer this question. This semester we learned to implement K-Means Clustering, Graph Algorithms, Decision Trees, Support Vector Machines and more. Additional reading discussed Factorization Machines and our group debated applying a Neural Network as well. We could have made a case to implement any of these algorithms. However, we decided on regression - particularly logistic regression - for multiple reasons.\n",
        "\n",
        "We are working with a large volume of data. As MIDS lecturer Dan Gillick remarked in the asynchronous lectures for Applied Machine Learning, regression can be very effective when you have copious amounts of data. We chose logistic regression over linear regression because our problem is inherently a binary classification problem with two response classes: the consumer clicks or does not click. Logistic regression predicts values mapped between 0 and 1, corresponding to a confidence of a sample being a member of a class. This makes logistic regression easily interpretable for binary classification.\n",
        "\n",
        "If a company hired us as consultants, we could interpret the coefficients of our model to explain what our model is producing to those who may lack Data Science expertise. The response predicted by the model is the probability that a consumer will click on an ad, given this information. This would enable our customer to say, “we only want to serve ads where the probability of a click is greater than X%.” In this scenario, the ease of interpretability and application outweighs the slight improvements from a more complex algorithm. \n"
      ]
    },
    {
      "metadata": {
        "id": "svN8gPpGqiSo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Theory with Toy Example"
      ]
    },
    {
      "metadata": {
        "id": "Udf9cb_bqmz8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this section, we walk through the steps of logistic regression using a toy example that resembles a subset of the fully pre-processed Criteo dataset.\n",
        "\n",
        "We have 4 features ('count0', count1', 'count2', 'hashed_category_0') and one label ('clicked'), with 5 total observations. The first three features are all count features which, based on the EDA, are mostly positive values. The feature 'hashed_category_0' resembles a one-hot encoded column from the original feature 'category_0'; therefore, the values will only be 0 or 1. The labels will also be 0 or 1, since they reflect the 'clicked' label of the full datset."
      ]
    },
    {
      "metadata": {
        "id": "F8PPGVQuqo2F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# build toy dataset resembling the Criteo dataset. last column is one-hot encoded\n",
        "# assume the data is already scaled\n",
        "toy_features = np.array(\n",
        "    [[0.1, 0.2,0.5,0.],\n",
        "    [2.5, 0.45,0.9,0.], \n",
        "    [3.2, 0.6,0.88,1.],\n",
        "    [1.1, 0.14,0.3,0.], \n",
        "    [0.6, 0.8,0.25,1.]])\n",
        "\n",
        "# build labels for the toy dataset, representing whether that set of features resulted in a click\n",
        "toy_labels = np.array([0,1,1,0,1])\n",
        "\n",
        "# Assign the feature names\n",
        "feature_names = ['bias', 'count0', 'count1', 'count2', 'hashed_category_0']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PXT3UfDmqstd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Using the toy dataset above, we would like to predict the probability that, given 4 features, a certain instance of an ad will result in a click. As mentioned in the introduction, we choose to use logistic regression because it is easily interpretable (in a business context, if this dataset were not anonymized) and is suitable for binary classification problems."
      ]
    },
    {
      "metadata": {
        "id": "oWaW6lrVqu_2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Explaining Prediction using Logistic Regression"
      ]
    },
    {
      "metadata": {
        "id": "TUKHzJy3QbnW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Logistic regression assumes that for a given set of input features $x$, we can determine a set of weights $w$ and a bias term $b$ that, when used to calculate output $y$ below, minimizes the amount of error between the probability of the predicted class and the true class. We predict $y$ using the formula below:"
      ]
    },
    {
      "metadata": {
        "id": "vLW6QmiLHvP2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$$y_w(x) = \\frac{1}{1+e^{-(wx+b)}}$$"
      ]
    },
    {
      "metadata": {
        "id": "l6JaJIEnH1F1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As mentioned earlier, this logistic regression function contains the linear regression term $L = wx+b$ within the exponent; that term gets fed into an “activation function” $\\frac{1}{1+e^{-L}}$, meaning it translates the results of the linear regression into a probability bounded by 0 and 1.\n",
        "\n",
        "Using the toy dataset, let's see what this equation would output for the first record if we simply guessed a vector of weights and bias. We'll assume that each of the 'count' features has a weight of 0.5, the one-hot encoded column has a weight of 2, and there is a bias of 0.2."
      ]
    },
    {
      "metadata": {
        "id": "OJ0Az_6oq6z1",
        "colab_type": "code",
        "outputId": "48ee2e8d-59b2-485d-a24f-024df05a3bbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Set weights and bias (pure guess to demonstrate predictions)\n",
        "# Bias is the first term in the vector\n",
        "guess_model = np.array([1, 0.5, 0.5, 0.5, 2])\n",
        "\n",
        "# Get the first record of the dataset and add a [1] in the front to represent the bias term.\n",
        "# This bias \"feature\" will be 1 for every observation, so that bias * 1 = bias\n",
        "first_record = np.insert(toy_features[0],0,1)\n",
        "first_record"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1. , 0.1, 0.2, 0.5, 0. ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "OOT8eMGIq_1U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "To calculate the value of the linear term, we take the dot product of the model and feature vectors. We then plug that number into the \"activation function\" mentioned above."
      ]
    },
    {
      "metadata": {
        "id": "5O9ixeQkq-qq",
        "colab_type": "code",
        "outputId": "f9bfa174-6750-4ca9-a5d8-7f2f40effd92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# calculate linear term\n",
        "linear_term = np.dot(first_record, guess_model)\n",
        "\n",
        "# plug into activation function to get predicted probability\n",
        "y_pred = 1/(1+np.exp(-linear_term))\n",
        "\n",
        "# print results\n",
        "print('The probability that the first observation would result in a click: ', y_pred)\n",
        "print('Assuming a cutoff of 0.5 for each class, this observation would be classified as ', round(y_pred))\n",
        "print('The true class is ', toy_labels[0])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The probability that the first observation would result in a click:  0.8021838885585818\n",
            "Assuming a cutoff of 0.5 for each class, this observation would be classified as  1.0\n",
            "The true class is  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fZV59UZ9rHKc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Unsurprisingly, our \"guessed\" model incorrectly predicted the class of this observation. We need machine learning to tell us what the model should actually look like instead of guessing!"
      ]
    },
    {
      "metadata": {
        "id": "civEF4Q9rI2-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Explaining the Cost Function"
      ]
    },
    {
      "metadata": {
        "id": "b2_gGUtiQhP3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So, how do we determine the optimal values of $w$ and $b$? We first need to define “optimal” by choosing the metric that will be used to compare outputs calculated with any given $w$ and $b$ against the true values. If we interpret the output as the probability that the label is 1 (clicked), then we can calculate loss $J$ as the negative log of the predicted value:"
      ]
    },
    {
      "metadata": {
        "id": "DbKl7NpBH9v4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$$J(w) = -log(y_w)$$ "
      ]
    },
    {
      "metadata": {
        "id": "XZcgE-D7IApm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This function can be used to represent loss because, as shown below, the loss decreases in magnitude as the predicted probability of label 1 increases. It is also a smooth function that only increases as the prediction approaches 0, meaning we can easily take the derivative to calculate the gradient."
      ]
    },
    {
      "metadata": {
        "id": "QOdW2DW3Blbv",
        "colab_type": "code",
        "outputId": "201d66a0-217c-45aa-a8c9-3c2352f48fa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "cell_type": "code",
      "source": [
        "# visualizing the cost function for the positive class\n",
        "# as predicted probability gets closer to the true label (1), loss approaches 0\n",
        "\n",
        "\n",
        "x = np.arange(0.01, 1, 0.01)\n",
        "y = -np.log(x)\n",
        "plt.plot(x,y)\n",
        "plt.title('-log(x) vs x')\n",
        "plt.xlabel('predicted probability')\n",
        "plt.ylabel('log loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niCQJBb6ISJJQ4IuIJAkFvohIkvj/hUBfCnBipTEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "FDOifTf8IcaZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we’ve calculated loss for the positive class (label = 1), we need to do the same for the negative class (label = 0). Since this is binary classification, a prediction of 0 would translate to a 0% probability of predicting the positive class, and therefore a 1 - 0 = 100% probability of predicting the negative class (which means there would be no loss). So if we replace the probability of predicting the positive class $y$, with the probability of predicting the negative class, $(1-y)$, we get a cost function of\n"
      ]
    },
    {
      "metadata": {
        "id": "VMeU23AUIeN0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$$J(w) = -log(1-y_w)$$ "
      ]
    },
    {
      "metadata": {
        "id": "ToNidWLCIi1Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The next step would be to calculate the gradient by taking the derivative of the cost function, but how do we combine the positive and negative cost functions into one? We only want to capture the loss compared to one label - whichever is the true label. Thanks again to binary classification, we can treat the true label $y_t$ as an activation term so that only the positive or negative cost is used:"
      ]
    },
    {
      "metadata": {
        "id": "fakBA7vgInU9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$$J(w) = -y_tlog(y_w)-(1-y_t)log(1-y_w)$$ "
      ]
    },
    {
      "metadata": {
        "id": "_As9u9mmIq_a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, if the true class $y_t$ is equal to 1, the second term will be canceled so we will only calculate the loss from the positive class, and vice versa. \n",
        "\n",
        "To scale this equation over multiple records and get the total loss for the dataset, we sum the loss and divide by total number of observations:"
      ]
    },
    {
      "metadata": {
        "id": "Y4elZ-0dIvS6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$$J(w) = -\\frac{1}{m}\\sum_{i=1}^m y_{t,i}log(y_{w,i})+(1-y_{t,i})log(1-y_{w,i}),$$"
      ]
    },
    {
      "metadata": {
        "id": "U66J7800rUwl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "where $y_{w,i}$ represents the function $y_w(x_i)$. \n",
        "\n",
        "Let's see how the cost function would look for our toy dataset, given the \"guessed\" model. We first need to generate all the predictions as we did with the first observations, and then take the negative log of both the values and (1-values) to get total log-loss."
      ]
    },
    {
      "metadata": {
        "id": "vv5lcsn9rZTq",
        "colab_type": "code",
        "outputId": "b7598780-21ab-4265-87b2-21e4f21758e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "# add the bias term in as the first \"feature\" for all of the observations\n",
        "bias = np.ones((5,1))\n",
        "toy_features_bias = np.hstack((bias, toy_features))\n",
        "\n",
        "# calculate all of the predictions\n",
        "y_pred = 1/(1+np.exp(-np.dot(toy_features_bias, guess_model)))\n",
        "\n",
        "# calculate the log-loss for each observation\n",
        "logloss = -toy_labels*np.log(y_pred) - (1-toy_labels)*np.log(1-y_pred)\n",
        "\n",
        "# display the results\n",
        "for i in range(len(toy_labels)):\n",
        "  print('Predicted probability: ', y_pred[i], 'Predicted class: ', round(y_pred[i]),\n",
        "        'True class: ', toy_labels[i], 'Log loss: ', logloss[i])\n",
        "\n",
        "# calculate the mean log-loss for the entire dataset\n",
        "print(\"Average log-loss for this model: \", logloss.mean())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted probability:  0.8021838885585818 Predicted class:  1.0 True class:  0 Log loss:  1.6204174099184512\n",
            "Predicted probability:  0.9490685297470565 Predicted class:  1.0 True class:  1 Log loss:  0.052274270390312086\n",
            "Predicted probability:  0.9952270198852368 Predicted class:  1.0 True class:  1 Log loss:  0.0047844071595555815\n",
            "Predicted probability:  0.8544576710630347 Predicted class:  1.0 True class:  0 Log loss:  1.9272883141507904\n",
            "Predicted probability:  0.9786474438389795 Predicted class:  1.0 True class:  1 Log loss:  0.02158381996264806\n",
            "Average log-loss for this model:  0.7252696443163514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ujB1HXGSreWx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It looks like our guessed model predicted that all of these ad instances would have resulted in a click, when actually only 3/5 did. It's not surprise that the two observations that were incorrectly predicted show the highest log loss. We also observe that the average log-loss for this guessed model is 0.73. The objective of logistic regression is to minimize that number, so we have a long way to go!"
      ]
    },
    {
      "metadata": {
        "id": "aP3H9Dofri8s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Explaining Regularization"
      ]
    },
    {
      "metadata": {
        "id": "fvPsvgiirl--",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order to prevent overfitting the training data, we can also introduce a regularization term that penalizes the cost function if the weights are too high. In Ridge regression, the penalty is determined by the squares of the weights, as well as a parameter $\\lambda$ that adjusts the overall severity and can be tuned manually. The total cost function with Ridge regression is \n",
        "\n",
        "\n",
        "$$J(w) = -\\frac{1}{m}\\big[\\sum_{i=1}^m y_{t,i}log(y_{w,i})+(1-y_{t,i})log(1-y_{w,i})\\big] + \\frac{\\lambda}{m}\\sum_{j=1}^{l}w_j^2$$\n",
        "\n",
        "\n",
        "where $l$ is the total number of features. The Lasso method of regularization is similar, but instead of trying to minimize all of the weights, it tries to remove as many features as possible to reduce overfitting. This can be done by taking the sum of the absolute value of the weights as a penalty, instead of the squares. The cost function for Lasso is \n",
        "\n",
        "\n",
        "$$J(w) = -\\frac{1}{m}\\big[\\sum_{i=1}^m y_{t,i}log(y_{w,i})+(1-y_{t,i})log(1-y_{w,i})\\big] + \\frac{\\lambda}{m}\\sum_{j=1}^{l}|w_j|$$\n",
        "\n",
        "\n",
        "Let's see what regularization does to the log loss on the toy dataset. We'll stick with Ridge regression in this example, since we don't have that many features to remove through Lasso:"
      ]
    },
    {
      "metadata": {
        "id": "F0-pkAGcrkpd",
        "colab_type": "code",
        "outputId": "82191c03-e68c-46e1-a540-8c148b6ee37d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# calculate the regularization term for the toy dataset\n",
        "# assume lambda = 0.1\n",
        "regParam = 0.1\n",
        "reg_term = regParam/(2*len(toy_labels))*np.sum(guess_model**2)\n",
        "\n",
        "# compare the total loss with and without regularization \n",
        "print('Loss: ', logloss.mean(), 'Loss w/ reg: ', logloss.mean() + reg_term)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss:  0.7252696443163514 Loss w/ reg:  0.7827696443163514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OEzZbsgqr2bQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Of course, the loss is higher with regularization because it's adding a penalty based on our weights. If we had lower weights across the board, the penalty would decrease; if we had one feature that stood out extremely high, the penalty would also increase by the square of that feature."
      ]
    },
    {
      "metadata": {
        "id": "Mxbcv4akr49y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Explaining the Gradient Function"
      ]
    },
    {
      "metadata": {
        "id": "lyWYXHqSI2yz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can now minimize the cost function by calculating the gradient $\\frac{\\delta}{\\delta w}J(w)$, which is used to update the weight vector $w$ with each iteration of gradient descent:"
      ]
    },
    {
      "metadata": {
        "id": "KTgWlbzSsBLT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$$\\frac{\\delta}{\\delta w}J(w) = \\frac{1}{m}\\sum_{i=1}^m x_i(y_{w,i} - y_{t,i})$$"
      ]
    },
    {
      "metadata": {
        "id": "g7OUSGGOsHUU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Adding in the derivative of the Ridge regularization term, we get\n",
        "\n",
        "$$\\frac{\\delta}{\\delta w}J(w) = \\frac{1}{m}\\sum_{i=1}^m x_i(y_{w,i} - y_{t,i}) + 2\\frac{\\lambda}{m}w$$"
      ]
    },
    {
      "metadata": {
        "id": "p6Na9XeesMhJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This gradient value is what's used at each iteration of gradient descent, where the weight vector that we had initially \"guessed\" moves closer and closer to the optimal weight vector that produces the least log-loss."
      ]
    },
    {
      "metadata": {
        "id": "JXF3BTU_sUGB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Explaining the Gradient Descent Update\n",
        "\n",
        "The size of the step change in the weight vector is dependent not only on the gradient, but also the learning rate, which is a hyperparameter that must be declared or optimally searched during development of a model. Bringing those two terms together, we get the formula for the new weight vector:\n",
        "\n",
        "$$w_{new} = w_{old} - \\alpha\\frac{\\delta}{\\delta w}J(w)$$\n",
        "\n",
        "Let's calculate the gradient for the toy dataset and see what our new weight vector will be. We can then compare the predictions and loss of the initial \"guessed\" weight vector to those of the updated one to confirm that the weights are moving closer to the right direction."
      ]
    },
    {
      "metadata": {
        "id": "DEJquGlMshaP",
        "colab_type": "code",
        "outputId": "90e376a3-8d88-4faf-da53-68e668188067",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "# calculate the gradient term for the toy dataset\n",
        "grad = (1/len(toy_labels) * (y_pred - toy_labels).dot(toy_features_bias)) + regParam * 2 * guess_model\n",
        "\n",
        "# set the learning rate equal to 0.1\n",
        "alpha = 0.1\n",
        "\n",
        "# calculate the new weights\n",
        "new_model = guess_model - alpha*grad\n",
        "\n",
        "# print the current weights, the step size change, and the new weights\n",
        "for i in range(len(guess_model)):\n",
        "  print(feature_names[i],' - ', 'old weight: ', guess_model[i], 'new weight: ', new_model[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bias  -  old weight:  1.0 new weight:  0.9484083089381422\n",
            "count0  -  old weight:  0.5 new weight:  0.47270583837342034\n",
            "count1  -  old weight:  0.5 new weight:  0.48525608285901917\n",
            "count2  -  old weight:  0.5 new weight:  0.4779589487834139\n",
            "hashed_category_0  -  old weight:  2.0 new weight:  1.9605225107255158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ow5Ok55asi_O",
        "colab_type": "code",
        "outputId": "54d7f39d-5e79-4e46-a33c-eda3fd51653c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "# Now let's compare the predictions using both the old and new model\n",
        "\n",
        "# first make the predictions using the new model\n",
        "y_pred_new = 1/(1+np.exp(-np.dot(toy_features_bias, new_model)))\n",
        "\n",
        "# print the comparison between predictions and true class\n",
        "for i in range(len(toy_labels)):\n",
        "  print('Old prediction: ', y_pred[i], 'New prediction: ', y_pred_new[i],\n",
        "        'True class: ', toy_labels[i])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Old prediction:  0.8021838885585818 New prediction:  0.7911232793781322 True class:  0\n",
            "Old prediction:  0.9490685297470565 New prediction:  0.9415139662749734 True class:  1\n",
            "Old prediction:  0.9952270198852368 New prediction:  0.9941377776095319 True class:  1\n",
            "Old prediction:  0.8544576710630347 New prediction:  0.8428658970431786 True class:  0\n",
            "Old prediction:  0.9786474438389795 New prediction:  0.9758788388319649 True class:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HQzIkpA_sk1q",
        "colab_type": "code",
        "outputId": "6021b9d1-0d9e-47a8-a3df-d9eb95ddda42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# and the logloss comparison\n",
        "\n",
        "# calculate the new log-loss for each observation\n",
        "logloss_new = -toy_labels*np.log(y_pred_new) - (1-toy_labels)*np.log(1-y_pred_new)\n",
        "reg_term_new = regParam/(2*len(toy_labels))*np.sum(new_model**2)\n",
        "\n",
        "# compare old and new model loss\n",
        "print('Old model: ', logloss.mean()+reg_term, 'New model: ', logloss_new.mean()+reg_term_new)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Old model:  0.7827696443163514 New model:  0.7557507875186116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RApDYwqRsm5T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With this model update, our average log-loss has decreased as we had hoped. If we continued to run this model update for multiple iterations, eventually we would near the minimum possible amount of loss. At that point the weight vector would represent the best model to fit the dataset.\n"
      ]
    },
    {
      "metadata": {
        "id": "2KdH9WGfPxQt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Section citations:\n",
        "* ML Cheatsheet: https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html\n",
        "* Machine Learning Medium: https://machinelearningmedium.com/2017/09/15/regularized-logistic-regression/"
      ]
    },
    {
      "metadata": {
        "id": "0fnm2kufJ3Cv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# III. EDA & Discussion of Challenges"
      ]
    },
    {
      "metadata": {
        "id": "pk2IqZA4MLRz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Notebook Setup\n",
        "\n",
        "We'll now get our notebook set up to run Spark."
      ]
    },
    {
      "metadata": {
        "id": "vWLhgDj2KAL_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Package Installations\n",
        "\n",
        "First we install some needed packages."
      ]
    },
    {
      "metadata": {
        "id": "wtXT71r7KEy_",
        "colab_type": "code",
        "outputId": "85aa508b-e132-4e96-a579-c09a80974449",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install -q findspark\n",
        "!pip install pyspark_dist_explore\n",
        "!pip install pyspark"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pip in /usr/local/lib/python3.6/dist-packages (19.0.3)\n",
            "Collecting pyspark_dist_explore\n",
            "  Downloading https://files.pythonhosted.org/packages/a6/25/873abc0e094edb982eddb4182e691f033dc7bd3144665a1c701853009f9d/pyspark_dist_explore-0.1.7-py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from pyspark_dist_explore) (3.0.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pyspark_dist_explore) (0.23.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyspark_dist_explore) (1.16.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyspark_dist_explore) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pyspark_dist_explore) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pyspark_dist_explore) (1.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pyspark_dist_explore) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pyspark_dist_explore) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->pyspark_dist_explore) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->pyspark_dist_explore) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->pyspark_dist_explore) (40.9.0)\n",
            "Installing collected packages: pyspark-dist-explore\n",
            "Successfully installed pyspark-dist-explore-0.1.7\n",
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/64/a1df4440483df47381bbbf6a03119ef66515cf2e1a766d9369811575454b/pyspark-2.4.1.tar.gz (215.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 215.7MB 69kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K    100% |████████████████████████████████| 204kB 29.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/47/9b/57/7984bf19763749a13eece44c3174adb6ae4bc95b920375ff50\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "srRrJZpfKN0j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Mount Google Drive to this Colab"
      ]
    },
    {
      "metadata": {
        "id": "QOn1fMhTKQcE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And we'll mount to our Google drive, where we are have our small samples of the dataset stored."
      ]
    },
    {
      "metadata": {
        "id": "eidZPAd-KXg3",
        "colab_type": "code",
        "outputId": "f72e21ac-77ac-4ffa-b321-db5eb5f5e607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a-S82c9jKTtC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Install Spark "
      ]
    },
    {
      "metadata": {
        "id": "yQoN4XfBKhHq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When running on Colab outside of a Dataproc cluster, we need to install and configure Spark."
      ]
    },
    {
      "metadata": {
        "id": "haMmLuefKopF",
        "colab_type": "code",
        "outputId": "72b91c9b-f4cd-4301-cbfa-d737fcd1a894",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cd /content/gdrive/'My Drive'/w261-final-project/notebook-data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/w261-final-project/notebook-data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ChAETkMXKqET",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# If needed uncomment the below to download the tar file\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.0-bin-hadoop2.7.tgz "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vu72GW2ZKu40",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import findspark\n",
        "import numpy as np\n",
        "import pyspark.sql\n",
        "import pyspark_dist_explore \n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "from itertools import combinations\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import Imputer, StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml.feature import OneHotEncoderEstimator, PolynomialExpansion\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql import types as t\n",
        "from pyspark.ml.linalg import SparseVector, DenseVector\n",
        "from pyspark.sql.functions import when\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.feature import FeatureHasher\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RYdW7-FqKmLx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Set Environment Variables"
      ]
    },
    {
      "metadata": {
        "id": "zaE7-B81K3kb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/gdrive/My Drive/w261-final-project/notebook-data/spark-2.4.0-bin-hadoop2.7/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kdu-2xQ1K7aT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Initialize Spark"
      ]
    },
    {
      "metadata": {
        "id": "cJpD54vhK4QQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "findspark.init()\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sQwQFiS4LdI8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loading Data and Sampling"
      ]
    },
    {
      "metadata": {
        "id": "BisQ1wu1LiUB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We looked at the original dataset and saw it was  45 million rows, which we knew would be difficult to work with.\n",
        "\n",
        "We chose to sample at 0.01% for a toy set to build our pipeline and 1% for a sample set to make hyperparameter decisions. The 1% decision came from section 5.3 of this [paper](https://www.dropbox.com/s/s4x7wp8gjsh021d/TISTRespPredAds-Chappelle-CTR-Prediction-2014Paper.pdf?dl+=0),  with the sentence starting, \"We found empirically...\"\n",
        "\n",
        "We wanted to sample randomly across the dataset rather than taking the first x rows, so we used the following Unix commands to stream over the data and subset:\n",
        "\n",
        "```\n",
        "cat train.txt | awk 'BEGIN {srand()} !/^$/ { if (rand() <= .0001) print $0}' > train_tiny.txt\n",
        "\n",
        "cat train.txt | awk 'BEGIN {srand()} !/^$/ { if (rand() <= .01) print $0}' > train_small.txt\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "XEVFE--hMGAt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Make Useful Headers\n",
        "\n",
        "The dataset contained one column indicating whether an advertisement was clicked, thirteen columns of count data and twenty-six columns of categorical data.  So we  created these headers for the input of our data."
      ]
    },
    {
      "metadata": {
        "id": "2XnzuW1aMG_n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "headers = ['clicked'] + [f\"count{i}\" for i,_ in enumerate(range(13))] + [f\"category{i}\" for i,_ in enumerate(range(26))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eKRXgWzQMbLL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load Files into Dataframes"
      ]
    },
    {
      "metadata": {
        "id": "YI_cw1PEMe-w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, the sampled data was loaded into a dataframe.\n",
        "\n",
        "The `sep=\"\\t\"` option was used in the import  to create columns from tab delineations. And the header list created above is passed in as the dataframe header."
      ]
    },
    {
      "metadata": {
        "id": "t0iBsxt9MaJC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tinyDF_raw = spark.read.load('/content/gdrive/My Drive/w261-final-project/notebook-data/train_tiny.txt', format='csv', sep=\"\\t\").toDF(*headers)\n",
        "# smallDF_raw = spark.read.load('/content/gdrive/My Drive/w261-final-project/notebook-data/train_small.txt', format='csv', sep=\"\\t\").toDF(*headers)\n",
        "# fullDF_raw = spark.read.load('gs://w261_jm/train.txt', format='csv', sep=\"\\t\").toDF(*headers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tdWAjCEFKZmV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order to maintain flexibility with our code, we assign the dataset we are currently working with the pseudonym of the full dataset."
      ]
    },
    {
      "metadata": {
        "id": "Sj48045iSHvB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fullDF_raw = tinyDF_raw"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bBA2Pc1KNHHU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Break Data into Train, Dev, Test Sets\n",
        "\n",
        "It was of course important to split the data so that we could explore models with a training set, evaluate on a dev set, and hold aside the test set in pristine condition so that we could later find out for real just how our final model (in this case 2 final models) would perform on formerly unseen data.\n",
        "\n",
        "We did this before our EDA because we did not want our held out test data to influence any design decisions we planned to make."
      ]
    },
    {
      "metadata": {
        "id": "NgiCFAy9MnRz",
        "colab_type": "code",
        "outputId": "e2ba7af9-5e75-4329-c669-403d3b458a91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "# number of rows should equal 45,840,617\n",
        "num_rows = fullDF_raw.count()\n",
        "\n",
        "# 75/15/10 train/dev/test split\n",
        "TRAIN_PORTION = 0.75\n",
        "DEV_PORTION = 0.15\n",
        "TEST_PORTION = 0.10\n",
        "\n",
        "# number of rows in each set\n",
        "TRAIN_COUNT = int(TRAIN_PORTION*num_rows) # 34,380,463\n",
        "DEV_COUNT = int(DEV_PORTION*num_rows) # 6,876,093\n",
        "TEST_COUNT = int(TEST_PORTION*num_rows) # 4,584,061\n",
        "\n",
        "# create split indices\n",
        "TRAIN_INDEX = TRAIN_COUNT\n",
        "DEV_INDEX = TRAIN_COUNT + DEV_COUNT\n",
        " \n",
        "# create indices to split on\n",
        "fullDF_raw_idx = fullDF_raw.withColumn(\"idx\", monotonically_increasing_id())\n",
        "fullDF_raw_idx.select('idx').createOrReplaceTempView('df')\n",
        "idx_rdd = spark.sql('select row_number() over (order by \"idx\") as num, * from df')\n",
        "fullDF_raw_idx = fullDF_raw_idx.join(idx_rdd, idx_rdd['idx'] == fullDF_raw_idx['idx'], how='left')\n",
        "\n",
        "# create train, dev, test dataframes based on index\n",
        "train_df_raw = fullDF_raw_idx.filter(f\"num < {TRAIN_INDEX}\")\n",
        "dev_df_raw = fullDF_raw_idx.filter(f\"num < {DEV_INDEX} and num >= {TRAIN_INDEX}\")\n",
        "test_df_raw = fullDF_raw_idx.filter(f\"num >= {DEV_INDEX}\")\n",
        "\n",
        "# drop the idx column\n",
        "train_df_raw.drop(*['idx','num'])\n",
        "dev_df_raw.drop(*['idx','num'])\n",
        "test_df_raw.drop(*['idx','num'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[clicked: string, count0: string, count1: string, count2: string, count3: string, count4: string, count5: string, count6: string, count7: string, count8: string, count9: string, count10: string, count11: string, count12: string, category0: string, category1: string, category2: string, category3: string, category4: string, category5: string, category6: string, category7: string, category8: string, category9: string, category10: string, category11: string, category12: string, category13: string, category14: string, category15: string, category16: string, category17: string, category18: string, category19: string, category20: string, category21: string, category22: string, category23: string, category24: string, category25: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "hneyEvBYNWPf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Verify and Preview Data\n",
        "\n",
        "We examine one row of training data to ensure that our split did not drop any original columns and to also preview what kind of data we'll be working with."
      ]
    },
    {
      "metadata": {
        "id": "caZpKtYlNQYT",
        "colab_type": "code",
        "outputId": "475f55e0-c8ee-4c2d-a099-5ec19f0318f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "cell_type": "code",
      "source": [
        "train_df_raw.show(n=1, truncate=False)\n",
        "print(f\"Ran in {(time.time() - start) // 60} minutes & {round((time.time() - start) % 60, 0)} seconds.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+------+------+------+------+------+------+------+------+------+------+-------+-------+-------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---+---+---+\n",
            "|clicked|count0|count1|count2|count3|count4|count5|count6|count7|count8|count9|count10|count11|count12|category0|category1|category2|category3|category4|category5|category6|category7|category8|category9|category10|category11|category12|category13|category14|category15|category16|category17|category18|category19|category20|category21|category22|category23|category24|category25|idx|num|idx|\n",
            "+-------+------+------+------+------+------+------+------+------+------+------+-------+-------+-------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---+---+---+\n",
            "|0      |null  |1     |3     |4     |14509 |null  |0     |43    |8     |null  |0      |null   |4      |5a9ed9b0 |04e09220 |71947b86 |bf9e41b6 |4cf72387 |7e0ccccf |1880ce04 |0b153874 |a73ee510 |3b08e48b |0fc6ec45  |8529d3b4  |94b87a00  |07d13a8f  |cae64906  |33a1f420  |776ce399  |e161d23a  |null      |null      |f22e0924  |ad3062eb  |be7c41b4  |ded4aac9  |null      |null      |0  |1  |0  |\n",
            "+-------+------+------+------+------+------+------+------+------+------+------+-------+-------+-------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---+---+---+\n",
            "only showing top 1 row\n",
            "\n",
            "Ran in 0.0 minutes & 6.0 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rKdWDgBpbkhl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It appeared that we would be dealing with integer values for our numeric columns and anonymized data for our categorical columns. It was important to observe that not all columns had data. Both the numeric and categorical columns had null values."
      ]
    },
    {
      "metadata": {
        "id": "KilKV38ISrjB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Examining Overall CTR of our Training Sample\n",
        "\n",
        "Next, we made some high-level explorations of our data. How many ads were clicked and how many were not clicked?"
      ]
    },
    {
      "metadata": {
        "id": "uXjxs_FOSzwJ",
        "colab_type": "code",
        "outputId": "95ca9113-13ce-4b24-856a-5078d7549365",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "# Examine overall CTR of our training sample\n",
        "overall = tinyDF_raw.select(\"clicked\") \\\n",
        "      .rdd \\\n",
        "      .map(lambda x: (x,1)) \\\n",
        "      .reduceByKey(lambda a,b: a+b) \\\n",
        "      .toDF(['clicks', 'howOften']) \\\n",
        "      .cache()\n",
        "\n",
        "overall.show(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+--------+\n",
            "|clicks|howOften|\n",
            "+------+--------+\n",
            "|   [0]|    3343|\n",
            "|   [1]|    1182|\n",
            "+------+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1Vc8AcvYS1pA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The overall CTR was 1182/4525 = 26.12%. This number is quite high and tells us that the data is likely downsampled. "
      ]
    },
    {
      "metadata": {
        "id": "0KpuFcsrORw9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Exploring Missing Data in the Dataset"
      ]
    },
    {
      "metadata": {
        "id": "NADhq2yvOpk8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we wanted to interrogate the null values we had seen in our preview. Is this positively or negatively associated with CTR?"
      ]
    },
    {
      "metadata": {
        "id": "IOa8gkHiOqG4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def report_columns_missing_a_value(row):\n",
        "  \"\"\"\n",
        "  Emit a record reporting whether a column's missing value\n",
        "  was associated with a click or a nonclick.\n",
        "  Returns column index as key and tuple of (nonclicks,clicks)\n",
        "  \"\"\"\n",
        "  clicked = int(row[0])\n",
        "  rest = row[1:]\n",
        "  for idx, val in enumerate(rest):\n",
        "    if val==None:\n",
        "      if clicked:\n",
        "        yield \"col\"+str(idx),(0,1)\n",
        "      else:\n",
        "        yield \"col\"+str(idx),(1,0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S7xMgPXJOteA",
        "colab_type": "code",
        "outputId": "290bce78-36fa-4e9e-e7eb-7b7ad430dfbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "cell_type": "code",
      "source": [
        "nobs=4525\n",
        "oCTR = 26.12\n",
        "missingsEDA = tinyDF_raw \\\n",
        "      .rdd \\\n",
        "      .flatMap(report_columns_missing_a_value) \\\n",
        "      .reduceByKey(lambda x,y: (x[0]+ y[0], x[1]+y[1])) \\\n",
        "      .mapValues(lambda x: ((round(float((x[0]+x[1])/nobs)*100,2)),(round(float(x[1]/(x[0]+x[1])*100),2)),(round(float(x[1]/(x[0]+x[1])*100-oCTR),2)))) \\\n",
        "      .map(lambda x: (int(x[0][3:]),x[1][0],x[1][1],x[1][2]))\\\n",
        "      .toDF(['column', 'percent_missing','CTR_when_missing','CTR_delta_when_missing'])\\\n",
        "      .sort(['column']) \\\n",
        "      .cache()\n",
        "\n",
        "\n",
        "missingsEDA.show(n=40, truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+---------------+----------------+----------------------+\n",
            "|column|percent_missing|CTR_when_missing|CTR_delta_when_missing|\n",
            "+------+---------------+----------------+----------------------+\n",
            "|0     |46.19          |19.9            |-6.22                 |\n",
            "|2     |21.88          |31.41           |5.29                  |\n",
            "|3     |22.28          |27.28           |1.16                  |\n",
            "|4     |2.9            |11.45           |-14.67                |\n",
            "|5     |22.63          |12.89           |-13.23                |\n",
            "|6     |4.46           |12.38           |-13.74                |\n",
            "|7     |0.02           |0.0             |-26.12                |\n",
            "|8     |4.46           |12.38           |-13.74                |\n",
            "|9     |46.19          |19.9            |-6.22                 |\n",
            "|10    |4.46           |12.38           |-13.74                |\n",
            "|11    |76.88          |25.29           |-0.83                 |\n",
            "|12    |22.28          |27.28           |1.16                  |\n",
            "|15    |3.27           |34.46           |8.34                  |\n",
            "|16    |3.27           |34.46           |8.34                  |\n",
            "|18    |12.35          |25.76           |-0.36                 |\n",
            "|24    |3.27           |34.46           |8.34                  |\n",
            "|28    |3.27           |34.46           |8.34                  |\n",
            "|31    |43.8           |26.54           |0.42                  |\n",
            "|32    |43.8           |26.54           |0.42                  |\n",
            "|33    |3.27           |34.46           |8.34                  |\n",
            "|34    |75.8           |26.01           |-0.11                 |\n",
            "|36    |3.27           |34.46           |8.34                  |\n",
            "|37    |43.8           |26.54           |0.42                  |\n",
            "|38    |43.8           |26.54           |0.42                  |\n",
            "+------+---------------+----------------+----------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "REKRvCJ-ObPz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This chart shows that missingness had a positive relationship with CTR in some columns and a negative relationship in others. For instance, when the 6th numerical field is missing (~20% of the time), such records have a CTR that is 13 points lower than the typical record. CTR is 6 points lower when the first or the 10th numerical field are missing (both of which were missing nearly half the time). Yet when the 3rd numerical value was missing (~20% of the time), the CTR was 5 points higher. "
      ]
    },
    {
      "metadata": {
        "id": "soTxJUqCc7JS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This chart also showed that we should expect all the columns to have at least one (and sometimes the majority) of records missing a value. Rather than simply imputing the median value, we decided that we would investigate how a missing value could help predict a click. "
      ]
    },
    {
      "metadata": {
        "id": "WfNFJbYEVXhe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Based on these findings, we decided to create “count{i}_missing” and \"category{i}_missing\" columns, to retain this signal. And then we imputed 0 for the numericals due to the description that these were counts, figuring that a missing value was more likely to indicate nothing having happened yet.\n"
      ]
    },
    {
      "metadata": {
        "id": "xZNqU9QbNjXL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Exploring the Categoricals\n",
        "\n",
        "We knew that our dataset contained thirteen numeric columns and twenty-six categorical columns. However, until the preview above, we did not know what information the categorical columns contained. The anonymized categorical data made it impossible to evaluate the individual categories of these columns. The categoricals could be unstructured tags or real columns with a consistent meaning from record to record."
      ]
    },
    {
      "metadata": {
        "id": "i2R-ZvlVNelP",
        "colab_type": "code",
        "outputId": "bdb087be-54d0-40d7-f67b-234ae53dcd8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "cell_type": "code",
      "source": [
        "def emit_category_impressions_and_clicks(row):\n",
        "  \"\"\"\n",
        "  Read in a row of our dataframe converted to RDD, and emit counts of\n",
        "  impressions and clicks.\n",
        "  \n",
        "  Returns:\n",
        "  \n",
        "  category_id, (1 for total impression count, 1 if this impression was clicked\n",
        "  or 0 if not)\n",
        "  \"\"\"\n",
        "  clicked = row[0]\n",
        "  if clicked == None:\n",
        "    clicked = 0\n",
        " \n",
        "  for count_idx, category_id in enumerate(row[1:]):\n",
        "    if category_id != None:\n",
        "      yield category_id, (1, int(clicked), {int(count_idx)})\n",
        "    \n",
        "def calculate_ctr(row):\n",
        "  \"\"\"Read in a row after counts have gone through the reducer to format our\n",
        "  statistics.\"\"\"\n",
        "  category_id, (total_impr, total_clicks, set_of_appearances) = row\n",
        "  return category_id, total_clicks, total_impr, round((float(total_clicks)/float(total_impr))*100,2), list(set_of_appearances)\n",
        "\n",
        "\n",
        "category_stats = train_df_raw.select(['clicked'] + [col for col in train_df_raw.columns if col.startswith(\"category\")]) \\\n",
        "      .rdd \\\n",
        "      .flatMap(emit_category_impressions_and_clicks) \\\n",
        "      .reduceByKey(lambda a,b: (a[0] + b[0], a[1] + b[1], {*a[2], *b[2]})) \\\n",
        "      .map(calculate_ctr) \\\n",
        "      .toDF(['category', 'clicks', 'impressions', 'CTR', 'Cols Appearing']) \\\n",
        "      .cache()\n",
        "\n",
        "category_stats.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+------+-----------+-----+--------------+\n",
            "|category|clicks|impressions|  CTR|Cols Appearing|\n",
            "+--------+------+-----------+-----+--------------+\n",
            "|5a9ed9b0|    81|        290|27.93|           [0]|\n",
            "|04e09220|    25|         53|47.17|           [1]|\n",
            "|71947b86|     3|          8| 37.5|           [2]|\n",
            "|bf9e41b6|     3|          8| 37.5|           [3]|\n",
            "|4cf72387|   139|        534|26.03|           [4]|\n",
            "|7e0ccccf|   338|       1382|24.46|           [5]|\n",
            "|1880ce04|     0|          1|  0.0|           [6]|\n",
            "|0b153874|   530|       2027|26.15|           [7]|\n",
            "|a73ee510|   829|       3027|27.39|           [8]|\n",
            "|3b08e48b|   178|        779|22.85|           [9]|\n",
            "|0fc6ec45|     0|          2|  0.0|          [10]|\n",
            "|8529d3b4|     3|          8| 37.5|          [11]|\n",
            "|94b87a00|     0|          2|  0.0|          [12]|\n",
            "|07d13a8f|   335|       1186|28.25|          [13]|\n",
            "|cae64906|     5|         14|35.71|          [14]|\n",
            "|33a1f420|     3|          8| 37.5|          [15]|\n",
            "|776ce399|    20|        183|10.93|          [16]|\n",
            "|e161d23a|     8|         18|44.44|          [17]|\n",
            "|f22e0924|     3|          8| 37.5|          [20]|\n",
            "|ad3062eb|   131|        476|27.52|          [21]|\n",
            "+--------+------+-----------+-----+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1z-ka29TYmmM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We checked to ensure that each and every categorical value appeared ONLY in the same column (see the Cols Appearing column in the table above). This turned out to be the case, and thus we felt confident that we had structured data and would not need to treat the categorical values for a given record as a “bag of words”.\n"
      ]
    },
    {
      "metadata": {
        "id": "xa74fdd3X2qX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Checking for Categorical Appearance in Multiple Categories"
      ]
    },
    {
      "metadata": {
        "id": "wX-adNlkmte5",
        "colab_type": "code",
        "outputId": "98d46b57-42e2-487b-9fad-aa18940a222f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "greater_than_zero = category_stats.select('Cols Appearing').rdd.filter(lambda x: len(x) > 0).count()\n",
        "\n",
        "print('The number of categories appearing in more than zero categories is: ', greater_than_zero)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of categories appearing in more than zero categories is:  20814\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q1FB2Ukfm8bn",
        "colab_type": "code",
        "outputId": "7515bbf0-c5ae-4bb1-a27a-6bdedf9c8563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "greater_than_one = category_stats.select('Cols Appearing').rdd.filter(lambda x: len(x) > 1).count()\n",
        "\n",
        "print('The number of categories appearing in more than one category is: ', greater_than_one)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of categories appearing in more than one category is:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P5TR2K3AnDwe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It appears that no values are shared across categories. This would simplify our use of interaction terms if we chose to incorporate them. Interaction terms are often used in regression to capture the effect that one independent variable has on another. "
      ]
    },
    {
      "metadata": {
        "id": "vcVOkLTHXhRL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Checking the Total Amount of Categories"
      ]
    },
    {
      "metadata": {
        "id": "50ZsvbtKX3Ye",
        "colab_type": "code",
        "outputId": "9a52145f-09c9-435c-903a-a3c79a5fd620",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "distinct_categories = category_stats.select('category') \\\n",
        "                                    .rdd \\\n",
        "                                    .flatMap(lambda x: x) \\\n",
        "                                    .collect()\n",
        "\n",
        "print('Number of rows in this subset: ', train_df_raw.count())\n",
        "print('Number of Distinct Categories: ', len(distinct_categories))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows in this subset:  3392\n",
            "Number of Distinct Categories:  20814\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PE9V7vBwZcTE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "From 3,392 rows, we had 20,814 categorical values. This was surprising and had implications for how we approached the categoricals in our model. Although one-hot encoding is a popular method to capturing categorical variables in a model, it may be impractical given how many different categories are in the dataset."
      ]
    },
    {
      "metadata": {
        "id": "dtKPBmhaNyK_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Checking How Many Unique Categoricals Appear in Each Column"
      ]
    },
    {
      "metadata": {
        "id": "mH7EtFHIWsWP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It appeared that the number of category values was many multiples of the number of rows in the subset. It became important to determine how many unique values there were in each category so we could understand their scope."
      ]
    },
    {
      "metadata": {
        "id": "8SOxQ38-Nvcc",
        "colab_type": "code",
        "outputId": "83c8a0f6-ffd9-4f73-f91d-07ecf3862eb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "cell_type": "code",
      "source": [
        "def emit_by_category_column(row):\n",
        "  category_hash, imprs, clicks, ctr, list_of_columns = row\n",
        "  for column_id in list_of_columns:\n",
        "    yield column_id, ({category_hash})\n",
        "\n",
        "category_stats_collengths = category_stats.rdd \\\n",
        "              .flatMap(emit_by_category_column) \\\n",
        "              .reduceByKey(lambda a,b: {*a, *b}) \\\n",
        "              .map(lambda x: (x[0], len(x[1]))) \\\n",
        "              .cache()\n",
        "\n",
        "\n",
        "category_stats_collengths.toDF(['Category Column', '# Unique Values']) \\\n",
        "              .show(30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+---------------+\n",
            "|Category Column|# Unique Values|\n",
            "+---------------+---------------+\n",
            "|              0|            101|\n",
            "|              1|            289|\n",
            "|              2|           2287|\n",
            "|              3|           1769|\n",
            "|              4|             36|\n",
            "|              5|              7|\n",
            "|              6|           1711|\n",
            "|              7|             58|\n",
            "|              8|              3|\n",
            "|              9|           1512|\n",
            "|             10|           1265|\n",
            "|             11|           2178|\n",
            "|             12|           1100|\n",
            "|             13|             22|\n",
            "|             14|           1166|\n",
            "|             15|           2003|\n",
            "|             16|              9|\n",
            "|             17|            720|\n",
            "|             20|           2097|\n",
            "|             21|              6|\n",
            "|             22|             13|\n",
            "|             23|           1210|\n",
            "|             18|            290|\n",
            "|             19|              3|\n",
            "|             24|             36|\n",
            "|             25|            931|\n",
            "+---------------+---------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gNZHWtaMXKKI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It appears that the distribution of the number of values across the different categories varies widely. Some categories have many possible values, while other categories have only a handful. "
      ]
    },
    {
      "metadata": {
        "id": "BNS9m-wtns_Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Calculating Interaction Terms"
      ]
    },
    {
      "metadata": {
        "id": "DZdtDLORn1-W",
        "colab_type": "code",
        "outputId": "a8bd3b5e-ea9e-4cad-b038-874e96ac0d90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "category_counts = category_stats_collengths.map(lambda x: x[1]).collect()\n",
        "\n",
        "list_of_products = []\n",
        "\n",
        "for i in range(len(category_counts)):\n",
        "  for j in range(len(category_counts)):\n",
        "    if j > i: # so we're only looking at new comparisons\n",
        "      try:\n",
        "        list_of_products.append(category_counts[i] * category_counts[j])\n",
        "        #print(f\"{category_counts[i]} * {category_counts[j]} = {category_counts[i] * category_counts[j]}\")\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "total_interaction_terms = sum(list_of_products)\n",
        "print(f\"All combinations of two categories would lead to {total_interaction_terms:,} interaction terms in addition to our {len(distinct_categories)} categorical terms.\")\n",
        "print(f\"If you're curious, that's a total of {total_interaction_terms + len(distinct_categories):,} terms.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All combinations of two categories would lead to 199,812,765 interaction terms in addition to our 20814 categorical terms.\n",
            "If you're curious, that's a total of 199,833,579 terms.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vAWxH0RYoXEj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Incorporating this many interaction terms into our model would be impractical given the number of terms and the amount of data we have available. "
      ]
    },
    {
      "metadata": {
        "id": "H2rf7b8cOFpI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Exploring the Numericals\n",
        "\n",
        "We were unable to determine the meanings of the 13 numerical fields, but understood that most of these were counts (e.g. total number of times the ad had been previously clicked on, or number of times this user had visited this website in some timeframe)\n",
        "\n",
        "A small number of records contained negative numbers, and we chose to treat these numbers as if they were also missing. "
      ]
    },
    {
      "metadata": {
        "id": "VIWfbXN2fJoU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# IV. Algorithm Implementation\n"
      ]
    },
    {
      "metadata": {
        "id": "utU-_Tb2G9Ar",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Homegrown Logistic Regression"
      ]
    },
    {
      "metadata": {
        "id": "W2glBVftG62c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "While we decided to use SparkML’s optimized algorithm for the full dataset, below we demonstrate a homegrown approach to parallelized logistic regression. We utilize mapping functions to handle record-level calculations (i.e. model predictions), and reduce functions to handle aggregations (i.e. loss and gradient calculations)."
      ]
    },
    {
      "metadata": {
        "id": "2ApDEzC6InD9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Like in the Algorithm Explanation system, there are a few steps and functions that need to be defined, such as the predict function, cost function, gradient function and gradient descent update. Since these steps are repeated with every iterations, we wrapped each step into Python functions."
      ]
    },
    {
      "metadata": {
        "id": "AEihkZ_pJKY0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Predict Functions"
      ]
    },
    {
      "metadata": {
        "id": "KSqNkZW4Jeon",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We define two helper functions below:\n",
        "* sigmoid is the \"activation function\" that maps the linear regression output to a value between 0 and 1\n",
        "* predictLR is the linear regression term that depends on the weights and bias\n",
        "\n",
        "From a parallelization standpoint, both of these functions are used in the mapping phase, as they get applied to each observation independently."
      ]
    },
    {
      "metadata": {
        "id": "qAexAD83HIga",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# helper functions for logistic regression\n",
        "\n",
        "def sigmoid(x):\n",
        "  \"\"\"\n",
        "  Converts probability to binary classifications\n",
        "  \n",
        "  \"\"\"  \n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def predictLR(x, model):\n",
        "  \"\"\"\n",
        "  Generates predictions for class labels based on inputs x and weights\n",
        "  \n",
        "  \"\"\"\n",
        "  return sigmoid(np.dot(x, model))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "313IDIi9JPoY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loss Functions"
      ]
    },
    {
      "metadata": {
        "id": "W-TOXeX_J8Rz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We define two more helper functions below:\n",
        "* rowLogLoss computes the log-loss for each individual observation by first calculating the predicted label, and then by comparing it to the true label\n",
        "* logLoss outputs the mean log-loss for the entire dataset\n",
        "\n",
        "Whie rowLogLoss gets applied at the mapping stage to each observation, logLoss ends with an aggregation step (calculating the mean) that occurs at the reducer."
      ]
    },
    {
      "metadata": {
        "id": "X5qCpLi2JSB3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate cross-entropy aka log-loss function J\n",
        "def rowLogLoss(row):\n",
        "    \"\"\"\n",
        "    Computes loss term for each observation\n",
        "    INPUT:  pred - predicted value\n",
        "            y - true value\n",
        "\n",
        "    \"\"\"  \n",
        "    pred, y = row\n",
        "    pos_class_cost = -y*np.log(pred)\n",
        "    neg_class_cost = -(1-y)*np.log(1-pred)\n",
        "    \n",
        "    return pos_class_cost+neg_class_cost\n",
        "\n",
        "# Calculate average log-loss across dataset\n",
        "def logLoss(augmentedRDD, model):\n",
        "  \"\"\"\n",
        "  Computes log-loss aka cross entropy for logistic regression model.\n",
        "  Args:\n",
        "      augmentedRDD - each record is a tuple of (bias + features_array , y)\n",
        "      model       - (array) model coefficients with bias at index 0\n",
        "  \"\"\"\n",
        "  loss = augmentedRDD.map(lambda x: (predictLR(x[0],model),x[1])) \\\n",
        "                      .map(rowLogLoss) \\\n",
        "                      .mean()\n",
        "\n",
        "  return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gxl8TrANJTLd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Gradient / Gradient Descent Update Functions"
      ]
    },
    {
      "metadata": {
        "id": "bTYdOc-oKkLZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We define the final two helper functions below:\n",
        "* GDUpdate includes the gradient calculation plus regularization, which first maps the single-observation gradient calculation and then reduces to get the mean. It then also performs the weight update step, which is a simple vector operation.\n",
        "* GradientDescent ties all of the helper functions together by calling nSteps iterations of GDUpdate and printing the loss on both the training and test datasets along the way. It also outputs the final model, which consists of the bias and weight terms that led to minimal loss."
      ]
    },
    {
      "metadata": {
        "id": "SK937WW-JcLa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# perform a single step of logistic regression gradient descent with regularization\n",
        "\n",
        "def GDUpdate(augmentedRDD, model, learningRate = 0.1, regType = None, regParam = 0.1):\n",
        "    \"\"\"\n",
        "    Perform one gradient descent step/update with ridge or lasso regularization.\n",
        "    Args:\n",
        "        augmentedRDD - tuple of (bias + features_array, y)\n",
        "        model       - (array) model coefficients with bias at index 0\n",
        "        learningRate - (float) defaults to 0.1\n",
        "        regType - (str) 'ridge' or 'lasso', defaults to None\n",
        "        regParam - (float) regularization term coefficient\n",
        "    Returns:\n",
        "        model   - (array) updated coefficients, bias still at index 0\n",
        "    \"\"\"\n",
        "    \n",
        "    new_model = None\n",
        "    \n",
        "    grad = augmentedRDD.map(lambda x: (x[0], predictLR(x[0],init_model),x[1])) \\\n",
        "                    .map(lambda x: np.dot(x[1]-x[2],x[0])) \\\n",
        "                    .mean()\n",
        "    if regType == 'ridge':\n",
        "      grad += regParam * np.append([0.0], model[1:])\n",
        "    elif regType == 'lasso':\n",
        "      grad += regParam * np.append([0.0], np.sign(model)[1:])\n",
        "    new_model = model - learningRate * grad\n",
        "\n",
        "    return new_model\n",
        "  \n",
        "# Perform full gradient descent with nSteps iterations\n",
        "def GradientDescent(trainRDD, testRDD, wInit, nSteps = 20, learningRate = 0.1,\n",
        "                         regType = None, regParam = 0.1, verbose = False):\n",
        "    \"\"\"\n",
        "    Perform gradient descent on logistic regression for n_steps. Outputs the training / test loss and model weights for each step\n",
        "    \"\"\"\n",
        "    # initialize lists to track model performance\n",
        "    train_history, test_history, model_history = [], [], []\n",
        "    \n",
        "    # perform n updates & compute test and train loss after each\n",
        "    model = wInit\n",
        "    for idx in range(nSteps):  \n",
        "      \n",
        "        # update the model\n",
        "        model = GDUpdate(trainRDD, model, learningRate, regType, regParam)\n",
        "        training_loss = logLoss(trainRDD, model) \n",
        "        test_loss = logLoss(testRDD, model)\n",
        "        \n",
        "        # console output if desired\n",
        "        if verbose:\n",
        "            print(\"----------\")\n",
        "            print(f\"STEP: {idx+1}\")\n",
        "            print(f\"training loss: {training_loss}\")\n",
        "            print(f\"test loss: {test_loss}\")\n",
        "            print(f\"Model: {[round(w,3) for w in model]}\")\n",
        "    return model, training_loss, test_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sAL_RdbGLm_h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Implementation"
      ]
    },
    {
      "metadata": {
        "id": "ZzAK0z5hLqJg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "While the toy explanation above provided many details for each calculation step, it was not repeatable or scalable. Now we can apply this scaled approach to a larger dataset. We used the subset of the Criteo dataset that has been preprocessed above. However, there are still a few additional preprocessing steps in order to generate interaction terms; **these steps will not be described here**, although they will be used. The full explanation will be in the next section, under **Method 1 - One Hot Encoding**, which is linked to a separate notebook."
      ]
    },
    {
      "metadata": {
        "id": "XujeQkKHNCa_",
        "colab_type": "code",
        "outputId": "452d49e9-d7af-43fc-face-77e1d5953c56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "##### ADDITIONAL PREPROCESSING FOR METHOD 1. SKIP THIS STEP, AS IT WILL BE DESCRIBED IN THE NEXT SECTION ####\n",
        "\n",
        "new_headers_lists = [['clicked']] + [[f\"count{i}\", f\"count{i}_null\"] for i in range(0,13)] + [[f\"category{i}\"]  for i,_ in enumerate(range(26))]\n",
        "new_headers = [header for shortlist in new_headers_lists for header in shortlist]\n",
        "\n",
        "COUNT_IMPUTE_VALUE = 0\n",
        "\n",
        "def flag_nulls_and_negatives(row):\n",
        "  new_row = [int(row[0])]\n",
        "  for i in range(13):\n",
        "    if row[f\"count{i}\"] is None or int(row[f\"count{i}\"]) < 0:\n",
        "      new_row.append(COUNT_IMPUTE_VALUE)\n",
        "      new_row.append(f\"cou{i}_null\")\n",
        "    else:\n",
        "      new_row.append(int(row[f\"count{i}\"]))\n",
        "      new_row.append(f\"cou{i}_notnull\")\n",
        "\n",
        "  for i in range(26):\n",
        "    if row[f\"category{i}\"] is None:\n",
        "      new_row.append(f\"category{i}_null\")\n",
        "    else:\n",
        "      new_row.append(row[f\"category{i}\"])\n",
        "\n",
        "  return new_row\n",
        "\n",
        "# replace negatives and nulls with 0\n",
        "train_df_processed = train_df_raw.rdd.map(flag_nulls_and_negatives).toDF(new_headers)\n",
        "dev_df_processed = dev_df_raw.rdd.map(flag_nulls_and_negatives).toDF(new_headers)\n",
        "test_df_processed = test_df_raw.rdd.map(flag_nulls_and_negatives).toDF(new_headers)\n",
        "\n",
        "# First we need to get quadratics, because we will lose the ability to identify\n",
        "# numeric columns when we hash.\n",
        "def quadratic_the_counts(row):\n",
        "  new_row = row.asDict()\n",
        "  quadratics = {}\n",
        "  for key, val in new_row.items():\n",
        "    if key.startswith('count') and not key.endswith('_null'):\n",
        "      quadratics[key+\"**2\"] = val**2\n",
        "      \n",
        "  return pyspark.sql.types.Row(**new_row, **quadratics)\n",
        "\n",
        "# add in the quadratics before one hot encoding\n",
        "train_ohe_quad = train_df_processed.rdd \\\n",
        "                  .map(quadratic_the_counts) \\\n",
        "                  .toDF()\n",
        "\n",
        "dev_ohe_quad = dev_df_processed.rdd \\\n",
        "                  .map(quadratic_the_counts) \\\n",
        "                  .toDF()\n",
        "\n",
        "test_ohe_quad = test_df_processed.rdd \\\n",
        "                  .map(quadratic_the_counts) \\\n",
        "                  .toDF()\n",
        "\n",
        "# Now we hash\n",
        "HASH_DIMENSIONS = 2048\n",
        "def hash_func(col_name, value, cat_bool, hash_dims):\n",
        "\n",
        "  value_to_return = 0\n",
        "  \n",
        "  if cat_bool == True:\n",
        "    string_to_hash = str(col_name) + str(value)\n",
        "    value_to_return = 1\n",
        "    \n",
        "  else:\n",
        "    string_to_hash = str(col_name)\n",
        "    value_to_return = int(value)\n",
        "\n",
        "  hash_total = 0\n",
        "\n",
        "  for char in string_to_hash:\n",
        "    hash_total += ord(char)\n",
        "    \n",
        "  hash_feature_idx = (hash_total**2) % hash_dims\n",
        "  \n",
        "  return (hash_feature_idx, value_to_return)\n",
        "\n",
        "\n",
        "def hash_values_and_interactions(row):\n",
        "  \n",
        "  new_row = {str(i):0 for i in range(HASH_DIMENSIONS)}\n",
        "  old_row = row.asDict()\n",
        "  \n",
        "  new_row['clicked'] = old_row['clicked']\n",
        "\n",
        "  for i in range(26): \n",
        "    \n",
        "      # Hash the categorical\n",
        "      this_cat = f\"category{i}\"\n",
        "\n",
        "      hash_idx, hash_val = hash_func(this_cat, old_row[this_cat], True, HASH_DIMENSIONS)\n",
        "      new_row[str(hash_idx)] += hash_val\n",
        "\n",
        "      for j in range(26):\n",
        "          if j < 13:\n",
        "              # Hash count_null interactions\n",
        "              this_catcount_inter = f\"{old_row[this_cat]}_BY_{old_row['count'+str(j)+'_null']}\"\n",
        "              hash_idx, hash_val = hash_func(this_catcount_inter, 1, True, HASH_DIMENSIONS)\n",
        "              new_row[str(hash_idx)] += hash_val\n",
        "          if i < j:\n",
        "              # Hash category interaction\n",
        "              this_cat_inter = f\"{old_row[this_cat]}_BY_{old_row['category'+str(j)]}\"\n",
        "              hash_idx, hash_val = hash_func(this_cat_inter, 1, True, HASH_DIMENSIONS)\n",
        "              new_row[str(hash_idx)] += hash_val\n",
        "\n",
        "      if i < 13:\n",
        "          # Hash the counts and nulls and quadratics\n",
        "\n",
        "          this_count = f\"count{i}\"\n",
        "          this_null = f\"count{i}_null\"\n",
        "          this_quad = f\"count{i}**2\"\n",
        "\n",
        "          hash_idx, hash_val = hash_func(this_count, old_row[this_count], False, HASH_DIMENSIONS)\n",
        "          new_row[str(hash_idx)] += hash_val\n",
        "\n",
        "          hash_idx, hash_val = hash_func(this_null, old_row[this_null], True, HASH_DIMENSIONS)\n",
        "          new_row[str(hash_idx)] += hash_val\n",
        "\n",
        "          hash_idx, hash_val = hash_func(this_quad, old_row[this_quad], False, HASH_DIMENSIONS)\n",
        "          new_row[str(hash_idx)] += hash_val\n",
        "\n",
        "          # If this isn't a quadratic\n",
        "\n",
        "          for j in range(13):\n",
        "              if i < j:\n",
        "                  new_val = old_row[f'count{i}'] * old_row[f'count{j}']\n",
        "                  hash_idx, hash_val = hash_func(f'count{i}_BY_count{j}', new_val, False, HASH_DIMENSIONS)\n",
        "                  new_row[str(hash_idx)] += hash_val\n",
        "  return new_row\n",
        "\n",
        "# each row adds an entry for common co-occurrences\n",
        "train_ohe_hashed = train_ohe_quad.rdd \\\n",
        "      .map(hash_values_and_interactions) \\\n",
        "      .toDF()\n",
        "test_ohe_hashed = test_ohe_quad.rdd \\\n",
        "      .map(hash_values_and_interactions) \\\n",
        "      .toDF()\n",
        "\n",
        "train_vector_assembler = VectorAssembler(inputCols = [str(i) for i in range(HASH_DIMENSIONS)], outputCol = 'features')\n",
        "train_vec = train_vector_assembler.transform(train_ohe_hashed)\n",
        "\n",
        "train_ohe_vec = train_vec.select('features', 'clicked')\n",
        "test_ohe_vec = train_vector_assembler.transform(test_ohe_hashed)\n",
        "\n",
        "# scale the features\n",
        "ohe_scaler = StandardScaler(withMean=True, inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "\n",
        "# Compute summary statistics by fitting the StandardScaler\n",
        "scalerModel_train = ohe_scaler.fit(train_ohe_vec)\n",
        "scalerModel_test = ohe_scaler.fit(test_ohe_vec)\n",
        "\n",
        "# Normalize each feature to have unit standard deviation.\n",
        "train_ohe_hashed_scaled = scalerModel_train.transform(train_ohe_vec)\n",
        "test_ohe_hashed_scaled = scalerModel_test.transform(test_ohe_vec)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyspark/sql/session.py:366: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
            "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "45uhjJ3KPJNj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here is where we implement the helper functions for the fully preprocessed dataset. We set 'verbose = True' in the GradientDescent function so we can watch as the training loss decreases with each iteration."
      ]
    },
    {
      "metadata": {
        "id": "cfgJOnjhHJvd",
        "colab_type": "code",
        "outputId": "c808df9e-f8a0-406c-ec49-2764103824eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "cell_type": "code",
      "source": [
        "# Grab tinyDF train and test data\n",
        "toyDF_train = train_ohe_hashed_scaled.select(['scaledFeatures','clicked'])\n",
        "toyDF_test = test_ohe_hashed_scaled.select(['scaledFeatures','clicked'])\n",
        "\n",
        "# add a bias 'feature' of 1 at index 0\n",
        "augmentedData_train = toyDF_train.rdd.map(lambda x: (list(np.append([1.0], x[0])), x[1])).cache()\n",
        "augmentedData_test = toyDF_test.rdd.map(lambda x: (list(np.append([1.0], x[0])), x[1])).cache()\n",
        "\n",
        "# Gather metadata\n",
        "n_feat = len(toyDF_train.select('scaledFeatures').take(1)[0][0])\n",
        "\n",
        "# Initialize weights to 0 and bias to 1\n",
        "init_model = list(np.insert(np.zeros(n_feat), 0, 1))\n",
        "\n",
        "# Model Hyperparameters\n",
        "n_steps = 5\n",
        "learning_rate = 0.01\n",
        "regType = 'ridge' # options: 'ridge', 'lasso', None\n",
        "regParam = 0.1\n",
        "verbose = True\n",
        "\n",
        "toy_model, toy_train_history, toy_test_history = GradientDescent(augmentedData_train, augmentedData_test, \n",
        "                                                             init_model, n_steps, learning_rate, regType, regParam, \n",
        "                                                             verbose)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------\n",
            "STEP: 1\n",
            "training loss: 1.0496107027015178\n",
            "test loss: 1.0401102772624178\n",
            "Model: [0.995, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "----------\n",
            "STEP: 2\n",
            "training loss: 1.0465944145142605\n",
            "test loss: 1.0373101113964198\n",
            "Model: [0.991, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "----------\n",
            "STEP: 3\n",
            "training loss: 1.0435996729099624\n",
            "test loss: 1.0345315694897745\n",
            "Model: [0.986, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.001, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.002, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.001, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "----------\n",
            "STEP: 4\n",
            "training loss: 1.0406265336936806\n",
            "test loss: 1.031774699722686\n",
            "Model: [0.981, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.001, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.002, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.001, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.002, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.002, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "----------\n",
            "STEP: 5\n",
            "training loss: 1.0376750509309778\n",
            "test loss: 1.0290395485450985\n",
            "Model: [0.976, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.001, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.002, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.002, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.003, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.002, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.001, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.002, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.002, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ArCA6EQYP96o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "At the end of 5 iterations, the model hasn't changed much yet, but we observe the loss moving towards the right direction. If we increase the number of iterations onto the order of hundreds, we will begin seeing appreciable results. Because this homegrown method is not fully optimized like SparkML, we decided to move forward with SparkML's LogisticRegression when running hundreds of iterations on the full datatset."
      ]
    },
    {
      "metadata": {
        "id": "Ex6Fde41HCGv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## SparkML Logistic Regression"
      ]
    },
    {
      "metadata": {
        "id": "p0d3ZtbPjB5B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We implemented Logistic Regression on two different sets of features. Those approaches are explained in companion notebooks, linked below."
      ]
    },
    {
      "metadata": {
        "id": "xmFB3kKYgCVa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Method 1: One-Hot Encoding"
      ]
    },
    {
      "metadata": {
        "id": "CpFyza1IgKzt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Method 2: Click-Through Rate"
      ]
    },
    {
      "metadata": {
        "id": "0Xw_25vLLai2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After initially developing those models, we tuned their hyperparameters in notebooks you can see below. In order to run more efficiently, we tuned hyperparameters using a 1% subset of the data.\n",
        "\n",
        "Hyperparameter tuning for Method 1\n",
        "\n",
        "Hyperparameter tuning for Method 2"
      ]
    },
    {
      "metadata": {
        "id": "NLLOLxJ6i8x-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluation of Results\n",
        "\n",
        "As we can see, using each categorical value's clickthrough rate instead of the hashed categorical value itself seemed to bring extra crucial information to the model, resulting in dramatically better results on prediction."
      ]
    },
    {
      "metadata": {
        "id": "sqt8O0EpfQ_7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# V. Application of Course Concepts & Lessons Learned"
      ]
    },
    {
      "metadata": {
        "id": "7JBjgjeQfWuP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Sampling**\n",
        "\n",
        "The focus of this course has been to develop machine learning algorithms that can scale to petabytes of data. Using this amount of data can be preferable because it can lead to large improvements over smaller datasets in terms of predictiveness. But it also increases the complexity of algorithm implementation. This additional complexity, along with other factors, makes it unwise to develop the algorithm at scale. Rather, the engineering should be done on smaller samples of data.\n",
        "\n",
        "The development cycle is faster on a smaller scale. Running an algorithm on petabytes of data may take hours, if not days. This is not conducive to quickly iterating and testing the development of a new model. It is also expensive to run an algorithm at scale. “At scale” necessitates using resources from multiple machines supplied by Google, Amazon or another cloud computing provider. Although the algorithm can be implemented on commodity hardware, costs can increase quickly when running multiple iterations. \n",
        "\n",
        "So it is important to minimize time using cloud resources. This is possible by developing the algorithm locally before running it in the cloud. But this will necessarily require a smaller dataset that a stand-alone computer can handle. The solution is to create a smaller dataset sampled from the total dataset that can then be run locally.\n",
        "\n",
        "It is not enough to simply take the top N rows of a dataset to use as a toy subset. In this Criteo display advertising dataset, the rows could be ordered by ad type, company or user. But the anonymization of the data prevents us from knowing with any certainty. So it is important to use random sampling to generate a subset that is representative of the overall dataset. We did this by generating a random number for each line and writing this line to a file if it fell below a certain threshold, depending on the size of the sample we wanted to create.\n",
        "\n",
        "The dataset for this project contained just under 46 million rows. We created two samples: a tiny subset with 0.01% of the data - about 4,600 rows - and a small subset that contained 1% - about 460,000 rows. These samples were used for some exploratory data analysis that guided our implementation and to develop the pipeline that generated our algorithm. \n",
        "\n",
        "By ensuring that our pipeline worked before running the notebook on Google Cloud Platform, our team saved time and money. Considering the number of iterations our algorithm went through - the errors that were corrected and ideas tested - and the amount of time it took to train the final algorithm at scale, we would have been unable to complete the project in four weeks if we used the entire dataset from the outset. "
      ]
    },
    {
      "metadata": {
        "id": "80EbY8LDfnRA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Scalability**\n",
        "\n",
        "Sampling enables us to work with a smaller dataset in order to develop our algorithm. But in order to run on the full dataset, we need to ensure that our algorithm is scalable. Scalability is the ability of an algorithm to handle larger amounts of data. The ideal is to have performance of an algorithm scale linearly with the amount of data it is trained on. In such a case, we can process more data by just adding more machines to process that data, and not have to make modifications to the algorithm.\n",
        "\n",
        "Algorithms that do not scale linearly often face bottlenecks or other points of contention that limit the ability to use additional resources. These bottlenecks slow down an algorithm’s ability to process data or make calculations or cause them to run out of memory. \n",
        "\n",
        "We developed our models on 1% of the dataset before running them on the full dataset. Wall time was an important factor for us as we scaled to the full dataset. We did not want our model to take hours and hours on end to run. But this was not as important as memory exhaustion. In earlier iterations, we hit memory limits when trying to train our algorithms at scale. We had to rethink and re-engineer how we were processing some features in order to avoid holding their values in memory. \n",
        "\n",
        "If we had not done this, our algorithm simply would not have run. We would have ran out of memory on each attempt and we would be left with error tracebacks instead of results."
      ]
    },
    {
      "metadata": {
        "id": "T7xjJtBvfqup",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**One-Hot Encoding**\n",
        "\n",
        "The Criteo dataset contained forty columns: a label/target variable that represents whether the advertisement was clicked, thirteen integer features, and twenty-six categorical features. Pre-processing the integer features was straightforward and focused on flagging negative or null values and standardizing. However, pre-processing the categorical features required more thought and discussion. \n",
        "\n",
        "Some algorithms - like decision trees - can use categorical features as is. But logistic regression is not one of those algorithms. So how could we interpret a non-numeric feature? Perhaps we could make a judgment based on the context of the category or our domain knowledge. But the values of these categories were anonymized so any knowledge or intuition we had could not be applied. We could encode each unique category for a feature as an integer. However, this applies an ordered relationship to each feature, which may not be valid. We needed a method of converting these categorical features into something interpretable by our algorithm if we wanted to include them.\n",
        "\n",
        "One-hot encoding is a method to represent categorical variables as numerics. It involves creating new columns for each possible value of a categorical feature and then for each row, assigning a binary value depending on that row’s original categorical value. For example, if one of our Criteo categories was “industry” and we had three possible values: technology, retail and finance, we would add three additional binary variables to our dataset. And for each row in the dataset, we would assign a “1” to the new variable to which that row belonged, and a “0” to the other variables.\n",
        "\n",
        "This adds many variables to our dataset. In our tiny (0.01%) sample, there were nearly 26,000 distinct values across the twenty-six categorical variables. However, without one-hot encoding, we would not have been able to incorporate the categoricals into our model, likely adversely affecting its predictiveness. A middle ground, which we ended up implementing, kept one-hot encoded variables but replaced categories with fewer than a certain number of observations with a single category name. This enabled us to use one-hot encoding while reducing the dimensionality of our feature space."
      ]
    },
    {
      "metadata": {
        "id": "AySYLZEfM7HQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**The Hashing Trick**\n",
        "\n",
        "We found the hashing trick to be a powerful way of reining in what would otherwise be too many features to compute. Additionally, it allows a model to consider categorical values of any size. It also runs very quickly, and is fairly easy to implement. We will consider this a valuable tool in the data scientist toolkit moving forward."
      ]
    },
    {
      "metadata": {
        "id": "6A-AX0WkftvO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Feature Selection**\n",
        "\n",
        "From our EDA, we found that the Criteo dataset contained thirteen numeric variables and twenty-six categorical variables. We performed this EDA on a 0.01% subset of the entire dataset, which amounted to 4,525 rows. Of the twenty-six categorical variables, we found that there were 26,000 distinct categories. We were interested in examining the interaction effects of these categories. An interaction effect captures whether the effect of an independent variable on the dependent variable changes depending on the value of another independent variable. In our case, although we did not know the meaning of the anonymized categories, we wanted to examine whether any interactions between the categories were significant.\n",
        "\n",
        "In regression, an interaction effect is represented as the product of two independent variables. If we were to include interaction effects for every possible combination of categories, we would have over 311 million terms for the 0.01% set. With only 4,525 rows in this dataset, we would have magnitudes more variables than data points. \n",
        "\n",
        "Creating a model with this many more variables than rows would have overfit our data set because there is too much opportunity for flexibility. Our model would have the power to fit superfluous, noisy or unrepresentative fluctuations in the data and achieve a very low error. But this model would not generalize well to a test set for these same reasons. Thus, it became necessary to prune irrelevant or only partially relevant features and simplify our model. If we had not removed these features, our model likely would have exhibited what was just described: overfitting, poor generalization and consequently, low performance.\n"
      ]
    },
    {
      "metadata": {
        "id": "zBDO5cQPfv6T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Bias/Variance Tradeoff**\n",
        "\n",
        "As detailed in the previous section, we could have used millions of variables in our model. We elected not to because, among other reasons, we would have overfit the training data. Another way to put this is that we would have reduced the bias of our model while increasing the variance.\n",
        "\n",
        "Variance refers to the amount by which the function estimated from the data changes when estimated with a different data set. Bias is the error of the function - the difference between the correct value we are trying to predict and the value we are actually predicting. Variance and bias compete with each other and result in a tradeoff. A model with high variance - and a large number of parameters - will very closely fit the data, but this close fit reduces the model’s bias. And the opposite is true as well. A model with low variance - and a low number of parameters - will not very closely fit the data, resulting in high bias. Both of these models are relatively easy to produce. The tradeoff is creating a model that simultaneously reduces both the variance and the bias. This can be achieved by reducing the total error of the model: the sum of the square of the bias and the variance. By optimizing the bias/variance tradeoff, we were able to create models that generalized well to the test data. Without this optimization, our models would have performed poorly on new data. \n"
      ]
    }
  ]
}